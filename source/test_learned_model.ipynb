{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import configparser\n",
    "\n",
    "#from absl import app\n",
    "#import nest_asyncio\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "import gower as gd\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(\"/home/tester/Desktop/TF/federated/tensorflow_federated/examples/simple_fedavg\")\n",
    "#sys.path.append(\"/home/tester/Desktop/TF/gower/gower\")\n",
    "#import simple_fedavg_tff\n",
    "#import gower_dist as gd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'fl' #fl\n",
    "RUN_NAME = 'FF0'\n",
    "#PRINT_SCR\n",
    "#BALANCE_DATA\n",
    "#TRAIN_SIZE\n",
    "#TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj = configparser.ConfigParser()\n",
    "config_obj.read('init/fl/'+MODE + '.ini')\n",
    "\n",
    "init = config_obj['SETUP']\n",
    "\n",
    "PRINT_SCR = bool(int(init['print_scr']))\n",
    "BALANCE_DATA = bool(int(init['balance_data']))\n",
    "TRAIN_SIZE = int(init['train_size'])\n",
    "TEST_SIZE = int(init['test_size'])\n",
    "\n",
    "if MODE == 'fl':\n",
    "    NUM_CLIENTS = int(init['num_clients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = tf.keras.models.load_model('results/' + MODE + '/' + RUN_NAME + '/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'cnl':\n",
    "    test_data = np.array(pd.read_csv('mats/cnl/' + RUN_NAME + '/test.csv', sep='\\s+', header=None))\n",
    "    test_labels = pd.read_csv('mats/cnl/' + RUN_NAME + '/test_labls.csv', header=None)\n",
    "else:\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    for i in range(0, NUM_CLIENTS):\n",
    "        test_data.append(np.array(pd.read_csv('mats/fl/' + RUN_NAME + '/' + str(i) + '_test.csv', sep='\\s+', header=None)))\n",
    "        test_labels.append(pd.read_csv('mats/fl/' + RUN_NAME + '/' + str(i) + '_test_labls.csv', sep='\\s+', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "  reconstructions = model(data)\n",
    "  return reconstructions\n",
    "\n",
    "def save_stats(predictions, labels, print_scr=True):\n",
    "\n",
    "  acc = \"{:.4f}\".format(accuracy_score(labels, predictions))\n",
    "  prec = \"{:.4f}\".format(precision_score(labels, predictions, zero_division=0))\n",
    "  rcl = \"{:.4f}\".format(recall_score(labels, predictions, zero_division=0))\n",
    "  f1 = \"{:.4f}\".format(f1_score(labels, predictions, zero_division=0))\n",
    "  roc = \"{:.4f}\".format(roc_auc_score(labels, predictions))\n",
    "\n",
    "  if print_scr:\n",
    "    print(\"Accuracy = \" + acc)\n",
    "    print(\"Precision = \" + prec)\n",
    "    print(\"Recall = \" + rcl) #sensitivity\n",
    "    print(\"F1 = \" + f1)\n",
    "    print(\"ROC_AUC = \" + roc)\n",
    "\n",
    "  return (acc + \" \" + prec + \" \" + rcl + \" \" + f1 + \" \" + roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6364\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.6373\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.7188\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.5824\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.5978\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.5941\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.6211\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.6780\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.6765\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n",
      "Accuracy = 0.6442\n",
      "Precision = 0.0000\n",
      "Recall = 0.0000\n",
      "F1 = 0.0000\n",
      "ROC_AUC = 0.5000\n"
     ]
    }
   ],
   "source": [
    "out = ''\n",
    "if MODE == 'cnl':\n",
    "    preds = predict(model, test_data)\n",
    "    a = save_stats(np.round(preds, decimals=0), test_labels.astype(int), True)\n",
    "    out = a + '\\n'\n",
    "else:\n",
    "    for i in range(0,NUM_CLIENTS):\n",
    "        preds = predict(model, test_data[i])\n",
    "        a = save_stats(np.round(preds, decimals=0), test_labels[i].astype(int), True)\n",
    "        out = out + a + '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0795357  -0.04372541 -0.11215445 ... -0.03973442 -0.06170009\n",
      "  -0.02951626]\n",
      " [-0.01693022 -0.08426588 -0.08953209 ...  0.00368234  0.00227912\n",
      "  -0.08559938]\n",
      " [ 0.00039241  0.09665311 -0.03616171 ... -0.10342579  0.01036007\n",
      "   0.0136985 ]\n",
      " ...\n",
      " [ 0.01536194 -0.03581058  0.1203389  ... -0.01302702 -0.01485945\n",
      "  -0.03442252]\n",
      " [ 0.0404333   0.09487855 -0.02929065 ... -0.06922565  0.03077221\n",
      "  -0.04699507]\n",
      " [-0.05709641 -0.02988226  0.0511932  ... -0.07617711 -0.12072495\n",
      "  -0.0644127 ]]\n"
     ]
    }
   ],
   "source": [
    "#first_layer_weights = model.layers[0].get_weights()[0]\n",
    "#print(first_layer_weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71c52cf81b3ce176a2dee7a24d4a513c23ffb6ff815f0de53fd7e035b7d8234"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
