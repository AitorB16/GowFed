{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqjnBbmi8BPM"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PTdfUcwp8Eru"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_JWOQsjxDDo"
      },
      "source": [
        "# Loading Remote Data in TFF\n",
        "\n",
        "---\n",
        "\n",
        "**NOTE**: This colab has been verified to work with the [latest released version](https://github.com/tensorflow/federated#compatibility) of the `tensorflow_federated` pip package, but the Tensorflow Federated project is still in pre-release development and may not work on `main`.\n",
        "\n",
        "In a production environment, the raw data for a federated computation is typically distributed across machines and requires special preprocessing and loading before it's usable.\n",
        "\n",
        "This tutorial describes how to load data stored in those remote locations with TFF's `DataBackend` and `DataExecutor` interfaces. But to keep the example simple, the dataset will exist entirely in memory and we'll smiulate the fetching as if the dataset was  partitioned over a network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNJf48UE8V-f"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/loading_remote_data\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.26.0/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.26.0/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/loading_remote_data.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sxoeaXPC8LF"
      },
      "source": [
        "## Before we start\n",
        "\n",
        "Before we start, please run the following to make sure that your environment is\n",
        "correctly setup. If you don't see a greeting, please refer to the\n",
        "[Installation](../install.md) guide for instructions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@title Set up open-source environment\n",
        "#@test {\"skip\": true}\n",
        "\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [],
      "source": [
        "#@title Import packages\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFgAEZID7_X"
      },
      "source": [
        "## Preparing the input data\n",
        "\n",
        "Let's begin by loading TFF's federated version of the EMNIST dataset from the built-in repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "och-dTLZoPHI"
      },
      "outputs": [],
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xkyMrciFYJc"
      },
      "source": [
        "We'll construct a preprocessing function to transform the raw examples in the EMNIST dataset from `28x28` images into `784`-element arrays. Additionally, the function will shuffle the individual examples, and rename the features from `pixels` and `label`, to `x` and `y` for use with Keras. We also throw in a `repeat` over the data set to run several epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kyiOMDO2GXKU"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "SHUFFLE_BUFFER = 100\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def map_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).map(map_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0prl3BhGWHm"
      },
      "source": [
        "Let's verify this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LTV06PafG0zV"
      },
      "outputs": [],
      "source": [
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "preprocessed_example_dataset = preprocess(example_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnMawhie2Nh"
      },
      "source": [
        "We'll use the EMNIST dataset to train a model by loading and preprocessing individual clients (emulating distinct partitions) through an implementation of `DataBackend`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n-grt-mHIYG"
      },
      "source": [
        "## Defining a `DataBackend`\n",
        "\n",
        "We need an instance of `DataBackend` to instruct TFF workers (these are the processes that run the client-side of federated computations) how to load and tranform the local data stored in remote locations. A `DataBackend` is a programmatic construct that resolves symbolic references, represented as application-specific URIs, to materialized payloads that downstream TFF operations can process. Specifically, a `DataBackend` object is wrapped by a `DataExecutor`, which queries the object when the TFF runtime encounters an operation that fetches the data.\n",
        "\n",
        "In this example, an Id to a client is encoded in a URI, which is parsed by our `DataBackend` definition to retrieve the corresponding client data, convert it to `tf.Dataset`, and then apply our `preprocess` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNFPMzCxl-4A"
      },
      "outputs": [],
      "source": [
        "class TestDataBackend(tff.framework.DataBackend):\n",
        "\n",
        "  async def materialize(self, data, type_spec):\n",
        "    client_id = int(data.uri[-1])\n",
        "    client_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "        emnist_train.client_ids[client_id])\n",
        "    return preprocess(client_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10G3IcFLl9QC"
      },
      "source": [
        "## Plugging the `DataBackend` into the `ExecutionContext`\n",
        "\n",
        "TFF computations are invoked by an `ExecutionContext` and in order for data URIs defined in TFF computations to be understood at runtime, a custom context must be defined that includes a pointer to the `DataBackend` we just created, so URIs can be properly resolved.\n",
        "\n",
        "The `DataBackend` works in tandem with `DataExecutor` to supply the executor with operable data that the executor can relay to requesting executors in order to complete a TFF computation. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OAKm2SxHnIA6"
      },
      "outputs": [],
      "source": [
        "def ex_fn(\n",
        "    device: tf.config.LogicalDevice) -\u003e tff.framework.DataExecutor:\n",
        "  return tff.framework.DataExecutor(\n",
        "      tff.framework.EagerTFExecutor(device),\n",
        "      data_backend=TestDataBackend())\n",
        "factory = tff.framework.local_executor_factory(leaf_executor_fn=ex_fn)\n",
        "ctx = tff.framework.ExecutionContext(executor_fn=factory)\n",
        "tff.framework.set_default_context(ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDPVBPsnKcg"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now we are ready to train a model in a federated fashion. Lets define a Keras model along with training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X_iXlL6corkR"
      },
      "outputs": [],
      "source": [
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(),\n",
        "  ])\n",
        "  \n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBfrmkq_pDd0"
      },
      "source": [
        "We can pass this TFF-wrapped definition of our model\n",
        "to a [Federated Averaging](https://arxiv.org/abs/1602.05629) algorithm by invoking the helper\n",
        "function `tff.learning.build_federated_averaging_process`, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SGORt8ImmcLH"
      },
      "outputs": [],
      "source": [
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT2r5Ayhpoaa"
      },
      "source": [
        "The `initialize` computation returns the initial state of the\n",
        "Federated Averaging process.\n",
        "\n",
        "To run a round of training, we need to construct a sample of data by organizing a sample of URI references as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f2igGrhfpmqY"
      },
      "outputs": [],
      "source": [
        "element_type = tff.types.StructWithPythonType(\n",
        "    preprocessed_example_dataset.element_spec,\n",
        "    container_type=collections.OrderedDict)\n",
        "dataset_type = tff.types.SequenceType(element_type)\n",
        "\n",
        "data_uris = [f'uri://{i}' for i in range(5)]\n",
        "data_handle = tff.framework.CreateDataDescriptor(arg_uris=data_uris, arg_type=dataset_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Pd90E206FT"
      },
      "source": [
        "Now we can round a round of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2nOmtKF7p4Vo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.11625), ('loss', 12.682653), ('num_examples', 2400), ('num_batches', 2400)]))])\n"
          ]
        }
      ],
      "source": [
        "state, metrics = iterative_process.next(state, data_handle)\n",
        "print('round 1, metrics={}'.format(metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrJ4a5aqD6D"
      },
      "source": [
        "And we can run a few more rounds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Grrk1VfnARXG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.12375), ('loss', 10.283665), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.17416666), ('loss', 7.7409034), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.27708334), ('loss', 5.7230663), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.39541668), ('loss', 4.3928266), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.47416666), ('loss', 3.5881217), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.54), ('loss', 3.1203246), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5995833), ('loss', 2.639122), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6141667), ('loss', 2.5395708), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
            "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.65625), ('loss', 2.2444942), ('num_examples', 2400), ('num_batches', 2400)]))])\n"
          ]
        }
      ],
      "source": [
        "NUM_ROUNDS = 11\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "  state, metrics = iterative_process.next(state, data_handle)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itjHBjJ6qOoS"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This concludes the tutorial. We encourage you to explore the other tutorials we've developed to learn about the many other features of the TFF framework."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "loading_remote_data.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
